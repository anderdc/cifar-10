{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Models\n",
    "here I will experiment with random forest models. I will use the simpler sklearn random forest, train it on PCA and unprocessed data, then I will use an XGboost random forest on PCA and unprocessed data. I'm scrapping the standardized data in this section because we learned from the logistic regression that standardized data doesn't provide much if any help in algorithms that don't using distance based calculations (i.e. PCA, KNN's). + random forests don't typical require feature scaling, so again training on standardized data will only provide subpar results.\n",
    "\n",
    "*Bonus: train on standardized data anyways to confirm or deny hypothesis*\n",
    "\n",
    "#### Sklearn Random Forest Classifier \n",
    "\n",
    "**PCA projected data (+ standardized)**\n",
    "\n",
    "\n",
    "**Unprocessed data**\n",
    "\n",
    "\n",
    "#### XGBoost Classifier\n",
    "\n",
    "**PCA projected data (+ standardized)**\n",
    "\n",
    "\n",
    "**Unprocessed data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch & prep all the data\n",
    "X, y = fetch_openml('CIFAR_10_small', as_frame=True, return_X_y=True, parser='auto')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=27)\n",
    "\n",
    "# standardize + pca\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)    # 'fit' a standardization for the train data so no test mean and std leaks to model\n",
    "X_test_std = scaler.transform(X_test)\n",
    "pca = PCA(n_components=3)\n",
    "X_train_pca = pca.fit_transform(X_train_std)   # decompose principal components from training data\n",
    "X_test_pca = pca.transform(X_test_std) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
